\documentstyle[12pt,fullpage]{article}

\title{A Brief Report on the Problem of Least Squares Fitting in MATLAB.}
\author{Greg Ward}
\date{25 August, 1993}

\begin{document}

\maketitle

The problem of least-squares fitting in MATLAB first arose in trying
to perform delay correction on plasma data for RCBF studies.  The
delay correction involves fitting an equation in four parameters
($\alpha$, $\beta$, $\gamma$, and $\delta$) to the average brain
activity across gray matter, $A(t)$:

\begin{equation}
A(t) = \alpha g(t+\delta) \otimes e^{-\beta t} + \gamma g(t+\delta)
\label{eq:fiteqn}
\end{equation}

where $g(t+\delta)$ is the dispersion-corrected plasma activity
shifted by the delay time $\delta$.  Since this problem was found to
be horribly overdetermined, we switched to the method recommended by
Dr. Meyer and used by Dr. Fujita's program: step through several
values of $\delta$, performing a three-parameter fit with respect to
$\alpha$, $\beta$, and $\gamma$, and choose from among the values of
$\delta$ that which give the best ($\alpha, \beta, \gamma$) fit.

The trouble, of course, is that if {\em one} fit is rather slow (and
this is the case), then fifteen fits will be {\em very} slow.  There
are two possible approaches to curing this speed problem: at the
lowest level, one can attempt to speed up evaluation of the function
to be fitted; or, at the highest level, one can attempt to speed up
the whole fitting algorithm.

We initially chose the first alternative, to speed up \verb|b_curve.m|
by rewriting in C various routines which it calls.  (\verb|b_curve.m|
is a MATLAB function that, given $\alpha$, $\beta$, $\gamma$,
$g(t+\delta)$, the time-scale in which the plasma activity was
resampled, and the frame start times and lengths, calculates the
right-hand-side of Eq. \ref{eq:fiteqn}.)

The most critical of these routines was frameint, which performs the
ubiquitous task of integrating a set of data across several frames and
dividing by the frame lengths.  (In previous implementations of the
RCBF analysis, this was not done; rather, the data set was simply
interpolated at the mid-frame time.  This achieves essentially the
same thing -- it approximates the average of the data set across the
frame -- much faster, but at the cost of accuracy.)  Upon re-writing
frameint in C, we observed a speed increase of roughly 50-fold, on the
order of the increase observed for other MATLAB-to-C translations.
This resulted in a speed increase of 2.5 times for \verb|b_curve|.
(Specifically: 100 calls to \verb|b_curve| using the MATLAB frameint
took ~16.8 sec; the same trial with \verb|b_curve| calling the CMEX
routine nframeint took ~6.6 sec.  Tests were done on the 50 MHz ursula
with no other users.  I also verified that the results of the two
version of \verb|b_curve| were identical.)

The next alternative is to rewrite the entirety of \verb|b_curve| in
C, rather than just bits and pieces of it.  This has been done (in
\verb|~greg/src/numanal/b_curve.c|), and it has been verified that the
results are within machine precision of the MATLAB implementation, but
we have not yet written a CMEX interface to test it from within MATLAB.

The other possible route to take is to replace the MATLAB minimisation
routine which we're using with a C equivalent.  This is desirable for
several reasons.

First, MATLAB comes with several flavours of minimisation/fitting
algorithms.  We have found that the faster ones (which are part of the
Optimization Toolbox) have a strong tendency to (apparently) go into
infinite loops when fitting \verb|b_curve|.  Thus, we have used the
slowest but most stable routine provided by MATLAB, \verb|fmins|.

Second, even the fastest routines (assuming they were stable) are not
terribly fast.  It could be that judicious tweaking of the "initial
guess" parameters; or use of analytic gradients of Eq. \ref{eq:fiteqn}
with respect to $\alpha$, $\beta$, and $\gamma$; or experimenting with
the different options the Optimization Toolbox provides could make
matters acceptable.  However, a general-purpose C routine would almost
certainly be much faster than anything written in MATLAB's interpreted
language.

Third, it would be very useful to have a general-purpose,
high-performance non-linear fitting routine for use within MATLAB for
{\em any} purpose, not just plasma delay correction.  Obviously, this
was what MATLAB's designers had in mind in releasing the Optimization
Toolbox.  However, the stability problems exhibited by the routines in
the Toolbox (leastsq and fminu in particular) give us some reason for
concern.

Unfortunately, the problem of finding a general-purpose, high-
performance fitting routine is fairly non-trivial; actually USING it
has turned out to be quite a hurdle.  I have found two routines which
might possibly fill our needs.  The first, \verb|conmin| (constrained
minimisation), was originally written in FORTRAN, and I translated it
to C using \verb|f2c|.  After some work, the result was fairly
readable C code (but with some lingering aftertaste of FORTRAN, most
notably a lot of \verb|goto|'s resulting in hard-to-follow spaghetti
code).  However, when I also translated the simple example program to
C and ran it, \verb|conmin| failed to find a minimum; when I compiled
and ran the original FORTRAN, it worked just fine.

Next, I tried \verb|lmdif|, a Levenberg-Marquardt implementation from
the CEPHES package of C numerical routines.  This too was originally
written in FORTRAN (as part of the MINPACK project), but translated
into C by the author of CEPHES.  The translation was much more
complete than my quick work on \verb|conmin|, and the example program
supplied with \verb|lmdif| worked fine.  \verb|lmdif| works by calling
a user-supplied routine called \verb|fcn|, which takes as input a
vector $x$ and returns a vector $fvec$.  $x$ in this case has three
elements ($\alpha$, $\beta$, and $\gamma$) and $fvec$ has 12 (for the
study I was working with as a test, this is the number of frames in
the first 30 seconds).

\verb|lmdif| then finds the value of $x$ that minimises the sum-of-squares of
$fvec$.  So, I wrote a function \verb|fcn| that calls the C version of
\verb|b_curve| and finds the difference between $A(t)$ and the 12 values
returned by \verb|b_curve| (one for each frame).  Then, I wrote a
program that sets up all the parameters to \verb|lmdif| much the same
as the \verb|lmdif| example program did, calls \verb|lmdif|, and
displays the results.  After a couple of attempts, it became apparent
that \verb|lmdif| was taking the initial $x$, exploring a few dozen
other possibilities, and eventually returned the exact same $x$ as was
passed to it.

In an effort to get anything meaningful out of \verb|lmdif|, I then
wrote a very simple \verb|fcn|, that takes a single value $x$, and
returns a single value $fvec = x^{2}$.  \verb|lmdif| did in fact come
close to $x=0$ as a minimum of the function quickly.  However, it did
not stop -- it got as low as $x ~ 10^{-60}$ before giving up after
performing too many calls to \verb|fcn| (the maximum number of calls
allowed by my main program was 400, on the recommendation of the
\verb|lmdif| example code).

In conclusion, then, it is clear that making use of even a finished,
well-polished least-squares fitting routine entails considerably more
than just writing a function to minimise and plugging all the right
parameters in.  This is definitely a project worthy of more time, but
such time is simply not available to us right now.

\end{document}
